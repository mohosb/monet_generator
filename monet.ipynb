{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Monet generator with GAN\n",
    "\n",
    "In the project I will attempt to create a deep neural network to generate images that are similar to Monet paintings. For this I will be using Generative Adversarial Networks (GANs). This is part of a Kaggle competition that is available through the following link: https://www.kaggle.com/competitions/gan-getting-started/overview\n",
    "\n",
    "This project is also available on my GitHub using the following link: https://github.com/mohosb/monet_generator\n",
    "\n",
    "## Data loading, preprocessing and EDA\n",
    "\n",
    "In the following section we create a dataset and dataloader object to load and preprocess the images, so we can take a look at them and later, feed them to the neural networks.\n",
    "\n",
    "There are exactly 300 images of Monet paintings and every image is 256x256 pixels with RGB channels. This is a reasonable images size. Not too big to require and incredibly large model but large enough so that we can clearly see what the images are about."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shutil"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": "class UnlabeledImageFolder(torch.utils.data.Dataset):\n    def __init__(self, root, transform):\n        self.root = root\n        self.transform = transform\n        self.all_images = sorted(os.listdir(root))\n\n    def __len__(self):\n        return len(self.all_images)\n\n    def __getitem__(self, idx):\n        image = Image.open(os.path.join(self.root, self.all_images[idx])).convert('RGB')\n        return self.transform(image)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-06T09:04:28.742513Z",
     "iopub.execute_input": "2023-06-06T09:04:28.743328Z",
     "iopub.status.idle": "2023-06-06T09:04:28.754893Z",
     "shell.execute_reply.started": "2023-06-06T09:04:28.743287Z",
     "shell.execute_reply": "2023-06-06T09:04:28.753570Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train_transform = torchvision.transforms.Compose([\n    torchvision.transforms.RandomHorizontalFlip(),\n    torchvision.transforms.ToTensor()\n])\n\ntrain_ds = UnlabeledImageFolder(root='/kaggle/input/gan-getting-started/monet_jpg', transform=train_transform)\ntrain_loader = torch.utils.data.DataLoader(train_ds, batch_size=50, shuffle=True, num_workers=0)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-06T09:04:30.885750Z",
     "iopub.execute_input": "2023-06-06T09:04:30.886454Z",
     "iopub.status.idle": "2023-06-06T09:04:30.917146Z",
     "shell.execute_reply.started": "2023-06-06T09:04:30.886420Z",
     "shell.execute_reply": "2023-06-06T09:04:30.916274Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(len(train_ds))\nprint(train_ds[0].shape)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-06T09:04:33.262713Z",
     "iopub.execute_input": "2023-06-06T09:04:33.263082Z",
     "iopub.status.idle": "2023-06-06T09:04:33.327820Z",
     "shell.execute_reply.started": "2023-06-06T09:04:33.263052Z",
     "shell.execute_reply": "2023-06-06T09:04:33.326757Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "plt.imshow(torchvision.utils.make_grid(next(iter(train_loader))[:16], nrow=4, padding=10).permute(1, 2, 0));",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-06T09:04:35.921556Z",
     "iopub.execute_input": "2023-06-06T09:04:35.921928Z",
     "iopub.status.idle": "2023-06-06T09:04:36.873205Z",
     "shell.execute_reply.started": "2023-06-06T09:04:35.921899Z",
     "shell.execute_reply": "2023-06-06T09:04:36.872373Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model building and training\n",
    "\n",
    "We will create a discriminator, which is a binary classifier that tries to differentiate generated fake images from real ones. This model will gradually decrease the spacial dimensions while increasing the number of channels, using residual blocks with instance normalization and convolutions.\n",
    "\n",
    "The other model will be the generator, that tries to create images from latent space representations that can fool the discriminator, so that it classifies the fake images as real ones. This way, the generator should learn the distribution of the original dataset. The generator model will contain residual blocks that gradually increase the spacial dimension and decrease the channel dimensions until we get an image of desired size. The up sampling blocks use instance normalizations and transposed convolutions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": "class DownSampleBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, depth_factor=1):\n        assert depth_factor >= 1\n        super().__init__()\n        layers = [\n            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.InstanceNorm2d(out_channels, affine=True),\n            nn.LeakyReLU(0.2, True)\n        ]\n        for _ in range(depth_factor - 1):\n            layers += [\n                nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n                nn.InstanceNorm2d(out_channels, affine=True),\n                nn.LeakyReLU(0.2, True)\n            ]\n        self.layers = nn.Sequential(*layers)\n        self.skip = nn.Conv2d(in_channels, out_channels, kernel_size=2, stride=2, padding=0, bias=False)\n\n    def forward(self, x):\n        s = x.clone()\n        x = self.layers(x)\n        x = x + self.skip(s)\n        return x\n\n\nclass UpSampleBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, depth_factor=1):\n        assert depth_factor >= 1\n        super().__init__()\n        layers = [\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.InstanceNorm2d(out_channels, affine=True),\n            nn.ReLU(True)\n        ]\n        for _ in range(depth_factor - 1):\n            layers += [\n                nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n                nn.InstanceNorm2d(out_channels, affine=True),\n                nn.ReLU(True)\n            ]\n        self.layers = nn.Sequential(*layers)\n        self.skip = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, padding=0, bias=False)\n\n    def forward(self, x):\n        s = x.clone()\n        x = self.layers(x)\n        x = x + self.skip(s)\n        return x\n\n\nclass Discriminator(torch.nn.Module):\n    def __init__(self, num_blocks=1, depth_factor=1, width_factor=1):\n        assert num_blocks >= 1 and depth_factor >= 1 and width_factor >= 1\n        super().__init__()\n        num_channels = [16] + [2 ** (4 + i) * width_factor for i in range(num_blocks - 1)]\n        self.head = nn.Conv2d(3, num_channels[0], kernel_size=3, stride=1, padding=0, bias=False)\n        self.blocks = nn.Sequential(\n            *[DownSampleBlock(num_channels[i - 1], num_channels[i], depth_factor) for i in range(1, len(num_channels))]\n        )\n        self.tail = nn.Sequential(\n            nn.Linear(num_channels[-1], 1),\n            nn.Sigmoid()\n        )\n        self.init_parameters()\n\n    def forward(self, x):\n        x = self.head(x)\n        x = self.blocks(x)\n        x = torch.mean(x, (2, 3))\n        x = self.tail(x)\n        return x\n\n    @torch.no_grad()\n    def init_parameters(self):\n        for m in self.modules():\n            class_name = m.__class__.__name__\n            if class_name.find('Conv') != -1:\n                torch.nn.init.normal_(m.weight, 0.0, 0.02)\n            elif class_name.find('BatchNorm') != -1 or class_name.find('InstanceNorm') != -1:\n                torch.nn.init.normal_(m.weight, 1.0, 0.02)\n                torch.nn.init.constant_(m.bias, 0)\n\n\nclass Generator(torch.nn.Module):\n    def __init__(self, latent_size, img_size, depth_factor=1, width_factor=1):\n        super().__init__()\n        self.latent_size = latent_size\n        num_channels = [16] + [2 ** (4 + i) * width_factor for i in range(int(np.log2(img_size)))]\n        num_channels = num_channels[::-1]\n        self.head = nn.ConvTranspose2d(latent_size, num_channels[0], kernel_size=1, stride=1, padding=0, bias=False)\n        self.blocks = nn.Sequential(\n            *[UpSampleBlock(num_channels[i - 1], num_channels[i], depth_factor) for i in range(1, len(num_channels))]\n        )\n        self.tail = nn.Sequential(\n            nn.ConvTranspose2d(num_channels[-1], 3, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.Sigmoid()\n        )\n        self.init_parameters()\n\n    def forward(self, x):\n        x = self.head(x[..., None, None])\n        x = self.blocks(x)\n        x = self.tail(x)\n        return x\n\n    @torch.no_grad()\n    def init_parameters(self):\n        for m in self.modules():\n            class_name = m.__class__.__name__\n            if class_name.find('Conv') != -1:\n                torch.nn.init.normal_(m.weight, 0.0, 0.02)\n            elif class_name.find('BatchNorm') != -1 or class_name.find('InstanceNorm') != -1:\n                torch.nn.init.normal_(m.weight, 1.0, 0.02)\n                torch.nn.init.constant_(m.bias, 0)\n\n    @torch.no_grad()\n    def get_random_latent_vector(self, batch_size, device='cpu'):\n        return torch.randn(batch_size, self.latent_size, device=device)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-06T09:10:06.854285Z",
     "iopub.execute_input": "2023-06-06T09:10:06.854641Z",
     "iopub.status.idle": "2023-06-06T09:10:07.096550Z",
     "shell.execute_reply.started": "2023-06-06T09:10:06.854612Z",
     "shell.execute_reply": "2023-06-06T09:10:07.095138Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def train_gan(model_g, model_d, opt_g, opt_d, train_loader, num_epochs=1):\n    assert next(model_g.parameters()).device == next(model_d.parameters()).device\n    device = next(model_g.parameters()).device\n    loss_fn = torch.nn.BCELoss()\n    model_d.train()\n    model_g.train()\n\n    num_batches = len(train_loader)\n    for epoch_idx in range(num_epochs):\n        print(f'{epoch_idx + 1}/{num_epochs} epoch:')\n        for batch_idx, real_data in enumerate(train_loader):\n            opt_d.zero_grad()\n            opt_g.zero_grad()\n            batch_size = real_data.size(0)\n\n            real_data = real_data.to(device)\n            fake_data = model_g(model_g.get_random_latent_vector(batch_size, device))\n            labels_real = torch.ones(batch_size, 1, device=device)\n            labels_fake = torch.zeros(batch_size, 1, device=device)\n\n            # Train discriminator with real and fake data\n            output_real = model_d(real_data)\n            loss_d_real = loss_fn(output_real, labels_real)\n            loss_d_real.backward()\n            output_fake = model_d(fake_data.detach())\n            loss_d_fake = loss_fn(output_fake, labels_fake)\n            loss_d_fake.backward()\n            opt_d.step()\n            loss_d = loss_d_real + loss_d_fake\n            #acc_d = torch.cat([output_real.round() == labels_real, output_fake.round() == labels_fake]).cpu().numpy().mean()\n\n            # Train generator\n            output = model_d(fake_data)\n            loss_g = loss_fn(output, labels_real)\n            loss_g.backward()\n            opt_g.step()\n            #acc_g = (output.round() == labels_real).cpu().numpy().mean()\n\n            #print(f'\\t{batch_idx + 1}/{num_batches} batch - D loss: {loss_d.item():.4f}, G loss: {loss_g.item():.4f}, D accuracy: {acc_d:.4f}, G accuracy: {acc_g:.4f}')\n            print(f'\\t{batch_idx + 1}/{num_batches} batch - D loss: {loss_d.item():.4f}, G loss: {loss_g.item():.4f}')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-06T09:04:48.680263Z",
     "iopub.execute_input": "2023-06-06T09:04:48.680827Z",
     "iopub.status.idle": "2023-06-06T09:04:48.700340Z",
     "shell.execute_reply.started": "2023-06-06T09:04:48.680786Z",
     "shell.execute_reply": "2023-06-06T09:04:48.699099Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#device = torch.device('cpu')\n#device = torch.device('mps')\ndevice = torch.device('cuda')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-06T09:04:51.568391Z",
     "iopub.execute_input": "2023-06-06T09:04:51.568858Z",
     "iopub.status.idle": "2023-06-06T09:04:51.575081Z",
     "shell.execute_reply.started": "2023-06-06T09:04:51.568821Z",
     "shell.execute_reply": "2023-06-06T09:04:51.574072Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "generator = Generator(128, 256, depth_factor=2, width_factor=2).to(device)\ndiscriminator = Discriminator(6, depth_factor=1, width_factor=1).to(device)\n\noptimizer_g = torch.optim.AdamW(generator.parameters(), lr=0.001, betas=(0.5, 0.999))\noptimizer_d = torch.optim.AdamW(discriminator.parameters(), lr=0.001, betas=(0.5, 0.999))\n\ntrain_gan(generator, discriminator, optimizer_g, optimizer_d, train_loader, 10)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-06T09:10:58.757128Z",
     "iopub.execute_input": "2023-06-06T09:10:58.757485Z",
     "iopub.status.idle": "2023-06-06T09:11:52.290182Z",
     "shell.execute_reply.started": "2023-06-06T09:10:58.757456Z",
     "shell.execute_reply": "2023-06-06T09:11:52.289204Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    generator.eval()\n",
    "    latent_vector = generator.get_random_latent_vector(4, device)\n",
    "    plt.imshow(torchvision.utils.make_grid(generator(latent_vector).cpu(), nrow=1, padding=10).permute(1, 2, 0))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-06T09:12:00.683211Z",
     "iopub.execute_input": "2023-06-06T09:12:00.683560Z",
     "iopub.status.idle": "2023-06-06T09:12:01.034444Z",
     "shell.execute_reply.started": "2023-06-06T09:12:00.683533Z",
     "shell.execute_reply": "2023-06-06T09:12:01.033581Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discussion and conclusion\n",
    "\n",
    "Although this notebook does not contain everything, I tried several generator and discriminator model architectures, but every one of them failed. The generator model commonly collapsed and started to generate the exact same image regardless of the input latent vector, and the discriminator always beats the generator. First I tried to increase the depth and with of the generator to better compete with the discriminator, but I run out of computational resources very quicly. After that, I tried to reduce the discriminator depth and width, but then, it became so simple for the generator to fool the discriminator, that the generated images basically looked like some kind of noise.\n",
    "\n",
    "In conclusion, although currently the best generator can not even generate images that remotely look like paintings, I am certain that with a good hyperparameter configuration and a big enough generator-discriminator size, the problem could be solved."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": "NUM_IMAGES = 8000\n\nos.makedirs('./images', exist_ok=True)\n\ngenerator.eval()\nfor i in range(NUM_IMAGES):\n    latent_vector = generator.get_random_latent_vector(1, device)\n    img = generator(latent_vector).cpu()[0]\n    torchvision.utils.save_image(img, f'./images/{i}.jpg')\n    shutil.make_archive('images', 'zip', 'images')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-06T09:13:41.087007Z",
     "iopub.execute_input": "2023-06-06T09:13:41.087355Z",
     "iopub.status.idle": "2023-06-06T09:14:55.857804Z",
     "shell.execute_reply.started": "2023-06-06T09:13:41.087328Z",
     "shell.execute_reply": "2023-06-06T09:14:55.856828Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
